{
  "documents": [
    {
      "id": "modes",
      "title": "Session Modes",
      "tags": "focusgroup modes sessions",
      "content": "Session Modes Focusgroup offers three session modes that control how agents interact and provide feedback. Choose the mode that best fits your evaluation goals. Overview Mode Agents See Each Other Rounds Best For single No 1 Quick checks, specific questions discussion Yes Multiple Debates, exploring tradeoffs structured Yes (within phases) 4 phases Comprehensive evaluations Single Mode The simplest mode: all agents answer independently, without seeing each other's responses. [session] mode = &quot;single&quot; How It Works Focusgroup captures the tool's help output Each agent receives the help text + your question All agents respond independently (optionally in parallel) Responses are collected and displayed When to Use Quick sanity checks Gathering independent perspectives A/B testing different agent configurations Simple, focused questions Example [session] name = &quot;quick-check&quot; mode = &quot;single&quot; moderator = false [tool] command = &quot;git&quot; [[agents]] provider = &quot;claude&quot; [[agents]] provider = &quot;codex&quot; [questions] rounds = [ &quot;What's the most confusing aspect of this tool's help output?&quot;, ] Discussion Mode Agents can see and respond to each other, enabling debate and collaborative exploration. [session] mode = &quot;discussion&quot; How It Works Round 1: All agents respond to the initial question Round 2+: Agents see previous responses and can build on, agree with, or challenge them Moderator (optional): Synthesizes the discussion at the end When to Use Exploring tradeoffs between approaches When you want agents to challenge each other Building consensus on complex issues Getting nuanced, multi-perspective analysis Example [session] name = &quot;design-discussion&quot; mode = &quot;discussion&quot; moderator = true [tool] command = &quot;mytool&quot; [[agents]] provider = &quot;claude&quot; name = &quot;UX-Advocate&quot; system_prompt = &quot;Prioritize user experience and ergonomics.&quot; [[agents]] provider = &quot;claude&quot; name = &quot;Minimalist&quot; system_prompt = &quot;Advocate for simplicity and fewer options.&quot; [[agents]] provider = &quot;codex&quot; name = &quot;Power-User&quot; system_prompt = &quot;Represent users who want advanced features.&quot; [questions] rounds = [ &quot;What's your initial take on this tool's design?&quot;, &quot;Given the other perspectives, what tradeoffs should we prioritize?&quot;, &quot;Can we reach consensus on the top 3 improvements?&quot;, ] Structured Mode Guided feedback through four distinct phases, ensuring comprehensive and organized evaluation. [session] mode = &quot;structured&quot; The Four Phases Explore - Initial impressions and understanding What does this tool do? How does it fit into workflows? First impressions of the interface Critique - Issues, concerns, and problems What's confusing or unclear? What could cause errors? What's missing? Suggest - Recommendations and improvements Specific changes to make New features to add Documentation improvements Synthesize - Final summary and conclusions Key takeaways Priority recommendations Overall assessment How It Works All agents go through each phase in sequence Within each phase, agents see each other's responses (if running sequentially) Phase context accumulates—later phases build on earlier insights Moderator (recommended) provides final synthesis When to Use Comprehensive tool evaluations Generating structured feedback reports When you need organized, actionable feedback Formal design reviews Example [session] name = &quot;full-review&quot; mode = &quot;structured&quot; moderator = true [tool] command = &quot;memex&quot; [[agents]] provider = &quot;claude&quot; name = &quot;Claude-Sonnet&quot; [[agents]] provider = &quot;codex&quot; name = &quot;Codex&quot; [questions] rounds = [ &quot;Evaluate this knowledge base CLI tool for AI agent use.&quot;, ] [output] format = &quot;markdown&quot; save_log = true Moderator Any mode can include a moderator that synthesizes all feedback: [session] moderator = true # Optional: customize the moderator [session.moderator_agent] provider = &quot;claude&quot; The moderator: Identifies common themes across agents Highlights unique insights Notes disagreements and tensions Provides prioritized recommendations Choosing a Mode ┌─────────────────────────────────────────────────────────────┐ │ What do you need? │ ├─────────────────────────────────────────────────────────────┤ │ │ │ Quick answer to a specific question? │ │ └─→ single mode │ │ │ │ Explore tradeoffs or get debate? │ │ └─→ discussion mode │ │ │ │ Comprehensive, structured evaluation? │ │ └─→ structured mode │ │ │ │ Any mode + want a summary? │ │ └─→ Enable moderator = true │ │ │ └─────────────────────────────────────────────────────────────┘ See also: configuration for full config reference, exploration for letting agents run the tool. Parallel vs Sequential Within any mode, you can control whether agents run in parallel: [session] parallel_agents = true # Default: faster, independent responses parallel_agents = false # Sequential: each agent sees prior responses Parallel (default): Faster execution, independent perspectives Sequential : Agents build on each other, more conversational"
    },
    {
      "id": "cli-path-lookup",
      "title": "CLI Tool PATH Lookup Behavior",
      "tags": "focusgroup configuration troubleshooting",
      "content": "CLI Tool PATH Lookup Behavior Focusgroup invokes external CLI tools ( claude , codex ) via subprocess and relies on standard PATH resolution to locate them. How It Works When focusgroup needs to run an agent CLI: PATH lookup : Uses Python's shutil.which() to verify the command exists Subprocess execution : Runs the command directly via asyncio.create_subprocess_exec() Error on failure : Raises a clear error if the command is not found Error Messages If a CLI tool is not in PATH, you'll see errors like: Claude CLI not found. Is it installed and in PATH? Codex CLI not found. Is it installed and in PATH? Solutions Option 1: Ensure CLIs are in PATH Add the directory containing your CLI tools to your PATH environment variable: # Add to ~/.bashrc or ~/.zshrc export PATH=&quot;$HOME/.local/bin:$PATH&quot; # Verify it works which claude which codex Option 2: Use absolute paths in configuration If your CLIs are installed in non-standard locations, you can specify absolute paths in your focusgroup configuration file: [agents.claude] command = &quot;/opt/anthropic/bin/claude&quot; [agents.codex] command = &quot;/usr/local/bin/codex&quot; Technical Details The PATH lookup is implemented in two places: CLITool class ( src/focusgroup/tools/cli.py ): General CLI wrapper using shutil.which() Agent implementations ( src/focusgroup/agents/claude.py , codex.py ): Direct subprocess calls Both raise descriptive errors when commands are not found, helping users diagnose PATH issues quickly."
    },
    {
      "id": "configuration",
      "title": "Configuration Reference",
      "tags": "focusgroup configuration reference",
      "content": "Configuration Reference Focusgroup uses TOML configuration files for full session control. This document describes all available options. Complete Example [session] name = &quot;my-review&quot; mode = &quot;structured&quot; # single | discussion | structured moderator = true # Enable synthesis at the end parallel_agents = true # Query agents concurrently exploration = false # Let agents run the tool [session.moderator_agent] # Optional: customize the moderator provider = &quot;claude&quot; [tool] type = &quot;cli&quot; # cli | docs command = &quot;mytool&quot; # Command name or path help_args = [&quot;--help&quot;] # Args to get help output working_dir = &quot;/path/to/dir&quot; # Optional working directory [[agents]] provider = &quot;claude&quot; name = &quot;Claude-Sonnet&quot; system_prompt = &quot;You are a DevOps engineer...&quot; exploration = false # Per-agent exploration override [[agents]] provider = &quot;codex&quot; name = &quot;Codex&quot; [questions] rounds = [ &quot;What's your first impression of this tool?&quot;, &quot;What improvements would you suggest?&quot;, ] [output] format = &quot;markdown&quot; # json | markdown | text directory = &quot;./output&quot; # Where to save files save_log = true # Persist session log Section Reference [session] Controls overall session behavior. Field Type Default Description name string auto-generated Session identifier mode string &quot;single&quot; Session mode: single , discussion , or structured (see modes ) moderator bool false Enable a moderator agent to synthesize feedback parallel_agents bool true Query agents concurrently vs sequentially exploration bool false Allow agents to run tool commands (see exploration ) agent_timeout integer none Timeout in seconds for all agents (overrides defaults) [session.moderator_agent] Optional custom configuration for the moderator. If omitted, uses Claude CLI with default synthesis prompt. Field Type Default Description provider string &quot;claude&quot; claude or codex [tool] Specifies the tool being evaluated. Field Type Default Description type string &quot;cli&quot; Tool type: cli or docs command string required CLI command name or path to docs help_args array [&quot;--help&quot;] Arguments to get help output working_dir string current dir Working directory for tool execution path_additions array [] Additional directories to add to PATH for agents [[agents]] Define one or more agents. At least one agent is required. Field Type Default Description provider string required claude , codex , or custom provider name (see providers ) name string auto-generated Display name for this agent system_prompt string none Custom system prompt for this agent exploration bool false Enable tool exploration for this agent timeout integer none Agent timeout in seconds (overrides session default) [questions] Define the questions/prompts for the session. Field Type Default Description rounds array required List of questions (at least one) In single mode, only the first question is used. In discussion and structured modes, agents see all questions in sequence. [output] Configure session output. Field Type Default Description format string &quot;text&quot; Output format: json , markdown , or text directory string none Directory to save output files save_log bool true Whether to save session log for later review Minimal Config The smallest valid config: [tool] command = &quot;git&quot; [[agents]] provider = &quot;claude&quot; [questions] rounds = [&quot;What do you think of this tool?&quot;] Built-in Providers Provider Description Notes claude Anthropic Claude via claude CLI Requires claude CLI installed and authenticated codex OpenAI Codex via codex CLI Requires codex CLI installed Custom Providers You can define custom providers in ~/.config/focusgroup/providers.toml : [myagent] name = &quot;My Custom Agent&quot; command = &quot;my-agent-cli&quot; prompt_arg = &quot;--prompt&quot; # How to pass the prompt context_arg = &quot;--context&quot; # How to pass context (optional) model_arg = &quot;--model&quot; # How to specify model (optional) description = &quot;My custom CLI agent&quot; Then use in configs: [[agents]] provider = &quot;myagent&quot; name = &quot;Custom Agent&quot; Model Defaults When model is not specified: claude : Uses claude CLI's default model codex : Uses codex CLI's default model Custom providers: Use their CLI defaults"
    },
    {
      "id": "providers",
      "title": "Agent Providers Guide",
      "tags": "focusgroup providers agents",
      "content": "Agent Providers Guide Focusgroup supports multiple LLM providers via their CLI tools. This guide covers setup and best practices for each. Overview Provider CLI Tool Best For Claude (Anthropic) claude General feedback, nuanced analysis Codex (OpenAI) codex Code-focused feedback, OpenAI perspective All agents operate in CLI mode, invoking the actual CLI tools. This provides authentic agent behavior—the same way agents really use tools. Why CLI-Only? Focusgroup deliberately uses only CLI-based agents rather than API calls or SDK integrations. This is a core design philosophy, not a limitation. The Problem with Approximations When you call an LLM API directly or use an agent SDK, you're not talking to &quot;the agent&quot;—you're talking to a model through your own harness. You control the system prompt, tool configurations, safety parameters, and context window management. The result is an approximation of an agent, not the real thing. Many agent implementations are closed-source. We don't know exactly how Claude Code, Codex, or other CLI agents are configured internally: What system prompts shape their behavior? What safety layers filter their responses? How do they manage context and tool use? What defaults and optimizations are baked in? These details significantly affect how agents interact with tools—and they're invisible when you roll your own integration. Authentic Feedback from Real Customers Focusgroup exists to get feedback on tools designed for AI agents. The agents are the customers. When evaluating a CLI tool that agents will use, you want feedback from: The actual agent as its provider ships it With its real configuration and behaviors Using the same interface it would use in production You don't want feedback from a simulation you cobbled together using API calls—that feedback reflects your harness, not the agent's actual experience. CLI Tools as Ground Truth CLI tools like claude and codex represent the most accurate form of each agent as its provider intends it to operate. By invoking these tools directly, focusgroup captures: Authentic agent reasoning and interaction patterns Real-world tool use behavior Provider-intended safety and capability boundaries This means focusgroup feedback reflects what agents will actually do with your tool, not what a bespoke API integration might do. Claude (Anthropic) Invokes the actual claude CLI tool, providing authentic agent behavior. Setup: # Install Claude Code CLI # See: https://docs.anthropic.com/claude-code # Authenticate claude auth login Config: [[agents]] provider = &quot;claude&quot; name = &quot;Claude&quot; When to use Claude: Testing how agents actually interact with tools Getting feedback that reflects real-world agent usage When you want the agent to use its full toolset (file reading, web access, etc.) Codex (OpenAI) Invokes the OpenAI Codex CLI for code-focused feedback. Setup: # Install Codex CLI # See OpenAI's codex documentation # Authenticate codex auth Config: [[agents]] provider = &quot;codex&quot; name = &quot;Codex&quot; When to use Codex: Getting code-focused feedback Testing CLI tools that agents interact with programmatically When you want an OpenAI-based CLI agent perspective Mixing Providers A key strength of focusgroup is combining multiple providers for diverse perspectives: [[agents]] provider = &quot;claude&quot; name = &quot;Claude-1&quot; system_prompt = &quot;Focus on UX and ergonomics.&quot; [[agents]] provider = &quot;claude&quot; name = &quot;Claude-2&quot; system_prompt = &quot;Focus on correctness and edge cases.&quot; [[agents]] provider = &quot;codex&quot; name = &quot;Codex&quot; # Code-focused perspective System Prompts Customize agent perspectives with system prompts: [[agents]] provider = &quot;claude&quot; name = &quot;DevOps-Engineer&quot; system_prompt = &quot;&quot;&quot;You are an experienced DevOps engineer who values: - Reliability and operational simplicity - Clear error messages and debugging support - Consistent behavior across environments Focus on operational concerns when evaluating this tool.&quot;&quot;&quot; [[agents]] provider = &quot;claude&quot; name = &quot;Junior-Developer&quot; system_prompt = &quot;&quot;&quot;You are a junior developer who is: - Still learning CLI conventions - Easily confused by complex options - Appreciative of good documentation and examples Evaluate this tool from a beginner's perspective.&quot;&quot;&quot; See also: configuration for full config options, exploration for letting agents run tools. Troubleshooting Claude CLI Issues # Verify Claude CLI is installed claude --version # Test authentication claude &quot;Hello, can you respond?&quot; Codex CLI Issues # Verify Codex CLI is installed codex --version # Test authentication codex &quot;Hello&quot;"
    },
    {
      "id": "quick-start",
      "title": "Quick Start",
      "tags": "focusgroup getting-started",
      "content": "Quick Start Get feedback from AI agents on your CLI tools in under 2 minutes. Install # Option 1: Run directly with uvx (no install needed) uvx --from git+https://github.com/chriskd/focusgroup focusgroup --help # Option 2: Install as a tool uv tool install git+https://github.com/chriskd/focusgroup # Option 3: Clone and install for development git clone https://github.com/chriskd/focusgroup &amp;&amp; cd focusgroup uv pip install -e &quot;.[dev]&quot; Requires Python 3.11+ and uv . Authenticate Focusgroup uses CLI tools for agents. Set up the providers you want: # For Claude claude auth login # For Codex (OpenAI) codex auth See providers for detailed setup. Run focusgroup ask &quot;Is this help clear?&quot; -x &quot;mytool --help&quot; -n 1 Output: Agent-1 (claude): The help output is mostly clear, but I have a few suggestions: 1. The &quot;--format&quot; flag mentions &quot;json, text&quot; but doesn't explain the default 2. The synopsis shows [OPTIONS] but doesn't indicate which are required 3. Consider adding example commands at the bottom Overall: 7/10 - functional but could be more discoverable for new users. Session saved: ~/.local/share/focusgroup/logs/20260106-abc123.json That's it! You've consulted an AI agent about your tool's usability. More Examples # Ask 3 agents (default) for diverse perspectives focusgroup ask &quot;What improvements would help agents use this?&quot; -x &quot;mytool --help&quot; # Let agents actually run the tool (exploration mode) focusgroup ask &quot;Try common workflows&quot; -x &quot;mytool --help&quot; --explore # JSON output for piping focusgroup ask &quot;Review this&quot; -x &quot;mytool --help&quot; -o json | jq . # Context from a file focusgroup ask &quot;Review this API&quot; -x &quot;@README.md&quot; # Context from stdin cat docs.md | focusgroup ask &quot;What's missing?&quot; -x - # Dogfood: review focusgroup itself focusgroup demo Next Steps configuration - Config file reference for structured sessions modes - Session modes (single, discussion, structured) exploration - Letting agents run your tool providers - Agent provider setup and troubleshooting"
    },
    {
      "id": "json-formats",
      "title": "JSON Output Formats",
      "tags": "focusgroup json api output",
      "content": "JSON Output Formats Focusgroup uses two distinct JSON formats: the internal session log format for storage and the export format for reporting and piping. Session Log Format (Internal) Used by: ~/.local/share/focusgroup/logs/*.json This is the raw Pydantic model serialization, used for session persistence and internal operations. { &quot;id&quot;: &quot;abc123&quot;, &quot;name&quot;: &quot;My Session&quot;, &quot;tool&quot;: &quot;mytool&quot;, &quot;created_at&quot;: &quot;2026-01-06T10:30:00&quot;, &quot;completed_at&quot;: &quot;2026-01-06T10:32:00&quot;, &quot;mode&quot;: &quot;single&quot;, &quot;agent_count&quot;: 3, &quot;rounds&quot;: [ { &quot;round_number&quot;: 1, &quot;question&quot;: &quot;Is the help output clear?&quot;, &quot;responses&quot;: [ { &quot;agent_name&quot;: &quot;Agent-1&quot;, &quot;provider&quot;: &quot;claude-cli&quot;, &quot;model&quot;: null, &quot;prompt&quot;: &quot;...&quot;, &quot;response&quot;: &quot;The help output is clear...&quot;, &quot;timestamp&quot;: &quot;2026-01-06T10:31:00&quot;, &quot;duration_ms&quot;: 1500, &quot;tokens_used&quot;: 150 } ], &quot;moderator_synthesis&quot;: null } ], &quot;final_synthesis&quot;: null, &quot;tags&quot;: [&quot;review&quot;] } Key Fields id : Short UUID (8 chars) prompt : The full prompt sent to the agent (included in responses) provider : The CLI backend used ( claude-cli , codex-cli ) Direct Pydantic serialization via model_dump() Export Format (Reports) Used by: focusgroup ask -o json , focusgroup logs export --json This is a curated format designed for human readability and tooling consumption. { &quot;id&quot;: &quot;20260106-abc123&quot;, &quot;tool&quot;: &quot;mytool&quot;, &quot;mode&quot;: &quot;single&quot;, &quot;created_at&quot;: &quot;2026-01-06T10:30:00.000000&quot;, &quot;completed_at&quot;: &quot;2026-01-06T10:32:00.000000&quot;, &quot;is_complete&quot;: true, &quot;name&quot;: &quot;My Session&quot;, &quot;agent_count&quot;: 3, &quot;round_count&quot;: 1, &quot;rounds&quot;: [ { &quot;round_number&quot;: 1, &quot;question&quot;: &quot;Is the help output clear?&quot;, &quot;responses&quot;: [ { &quot;agent_name&quot;: &quot;Agent-1&quot;, &quot;provider&quot;: &quot;claude-cli&quot;, &quot;response&quot;: &quot;The help output is clear...&quot;, &quot;timestamp&quot;: &quot;2026-01-06T10:31:00.000000&quot;, &quot;model&quot;: &quot;claude-sonnet-4&quot;, &quot;duration_ms&quot;: 1500, &quot;tokens_used&quot;: 150, &quot;structured_data&quot;: null } ] } ], &quot;summary&quot;: { &quot;total_responses&quot;: 3, &quot;unique_providers&quot;: [&quot;claude-cli&quot;], &quot;total_tokens&quot;: 450, &quot;total_duration_ms&quot;: 4500, &quot;wall_time_seconds&quot;: 120.5 } } Key Differences from Session Log Aspect Session Log Export Format id Short UUID Display ID ( YYYYMMDD-uuid ) prompt Included in responses Omitted (use question) round_count Not present Computed field is_complete Not present Computed boolean summary Not present Aggregated statistics structured_data Optional per response Optional per response Summary Statistics The export format includes computed metrics: total_responses : Sum of all agent responses unique_providers : Deduplicated list of providers used total_tokens : Sum of tokens across all responses total_duration_ms : Sum of agent response times wall_time_seconds : Real elapsed time (if session complete) Choosing a Format Use Case Recommended Format Piping to jq Export ( -o json ) Archiving sessions Session log (automatic) CI/CD integration Export with --quiet Debugging prompts Session log (includes prompt ) Metrics collection Export (includes summary ) Future Compatibility Both formats may evolve. For programmatic consumers, we recommend: Parse fields you need, ignore unknown fields Check for presence of optional fields before accessing Use the export format for stable consumption A schema_version field may be added in future releases for breaking changes. Structured Feedback Data When using --schema , responses include a structured_data field with parsed JSON: { &quot;response&quot;: &quot;{\\&quot;rating\\&quot;: 4, \\&quot;reasoning\\&quot;: \\&quot;Good tool\\&quot;}&quot;, &quot;structured_data&quot;: { &quot;rating&quot;: 4, &quot;reasoning&quot;: &quot;Good tool&quot; } } See Structured Feedback Schemas for details on requesting structured responses. See Also Configuration Reference - Output format options Session Modes - How different modes affect output Structured Feedback Schemas - Request JSON responses"
    },
    {
      "id": "exploration",
      "title": "Exploration Mode",
      "tags": "focusgroup exploration security",
      "content": "Exploration Mode Exploration mode lets agents actually run the tool being evaluated, rather than just reading its help output. This provides more authentic feedback based on real interaction. Overview By default, focusgroup shows agents the tool's --help output and asks them to provide feedback. With exploration mode enabled, agents can: Run the tool with various arguments Explore subcommands Test edge cases Discover issues through actual use Enabling Exploration Via CLI focusgroup ask mx &quot;Try searching for deployment docs&quot; --explore Via Config [session] exploration = true # Enable for all agents # Or per-agent: [[agents]] provider = &quot;claude&quot; exploration = true # Just this agent can explore How It Works Context Enhancement : Agents receive instructions on how to run the tool Tool Access : CLI agents can execute the tool command Interactive Feedback : Agents explore, then report findings What Agents See With exploration enabled, agents receive additional context: ## Interactive Exploration **IMPORTANT**: You can and should run `mytool` commands to explore this tool before giving feedback! ### How to Explore 1. Try the basic help: mytool --help 2. Explore subcommands: mytool &lt;subcommand&gt; --help 3. Try common operations 4. Explore subcommands that interest you Requirements Exploration works with CLI-based agents (see providers for setup): Provider Exploration Support Claude Full support Codex Full support Example: Exploring a Search Tool [session] name = &quot;memex-exploration&quot; mode = &quot;single&quot; exploration = true moderator = true [tool] command = &quot;mx&quot; [[agents]] provider = &quot;claude&quot; name = &quot;Explorer-1&quot; [[agents]] provider = &quot;codex&quot; name = &quot;Explorer-2&quot; [questions] rounds = [ &quot;Explore this knowledge base tool. Try searching for various topics, then report what worked well and what was confusing.&quot;, ] Sample Session Output ## Explorer-1 (Claude) I explored the `mx` tool by running several commands: 1. `mx --help` - Good overview, clear subcommand list 2. `mx search &quot;deployment&quot;` - Found relevant docs quickly 3. `mx search &quot;nonexistent-topic&quot;` - Helpful &quot;no results&quot; message 4. `mx get docs/deployment.md` - Retrieved full content **What worked well:** - Search is fast and results are relevant - Error messages are clear **What was confusing:** - Unclear difference between `search` and `list` - No obvious way to see all available tags ## Explorer-2 (Codex) I tested the tool with various inputs: 1. Basic search worked well 2. Tried `mx add` but wasn't sure about required fields 3. `mx tree` gave a good overview of structure **Suggestions:** - Add examples to each subcommand's help - Show available tags in search results Security Considerations WARNING : Exploration mode grants agents significant system access. Read this section carefully before enabling exploration. What Permissions Are Granted CLI agents run with relaxed permission controls to enable tool exploration: Provider Permission Flags What This Means Claude --dangerously-skip-permissions Bypasses all permission prompts; agent can run any command without approval Codex (explore) --sandbox danger-full-access Removes sandbox restrictions; full filesystem and network access Codex (no explore) --full-auto Standard Codex safety checks apply What Agents Can Access With exploration mode enabled, CLI agents can: Filesystem : Read, write, and delete any files accessible to your user account Network : Make HTTP requests, download files, connect to services Shell Commands : Run arbitrary shell commands (git, curl, rm, etc.) Environment : Access environment variables, including secrets in your shell Other Tools : Invoke any CLI tools installed on your system Why These Permissions? Exploration mode exists to let agents actually use tools and give authentic feedback. An agent exploring a CLI tool needs to run that tool—which requires shell access. Sandbox restrictions would prevent agents from testing the very tool you want feedback on. This is a deliberate trade-off: useful exploration feedback requires real tool access . Recommendations 1. Run in Isolated Environments The safest approach is to run focusgroup exploration in an isolated environment: # Use a container docker run -it --rm -v $(pwd):/workspace myimage focusgroup ask mytool &quot;...&quot; --explore # Use a VM # Run focusgroup inside a disposable VM or devcontainer # Use a separate user account # Create a low-privilege account specifically for exploration 2. Review Tools Before Exploration Before enabling exploration for a tool: Understand what the tool can do (especially write operations) Check if the tool accesses sensitive data or services Consider using read-only tools first to test the setup 3. Limit What's in the Environment # Run with a clean environment to limit exposed secrets env -i PATH=&quot;$PATH&quot; HOME=&quot;$HOME&quot; focusgroup ask mytool &quot;...&quot; --explore # Or explicitly set only needed variables export MYTOOL_CONFIG=/path/to/config focusgroup ask mytool &quot;...&quot; --explore Risk Summary Scenario Risk Level Recommendation Exploring your own read-only tool in a dev container Low Safe to proceed Exploring any tool in a VM/container Low Safe to proceed Exploring read-only tools on your main system Medium Acceptable with caution Exploring write-capable tools on your main system High Use isolated environment Running exploration with sensitive secrets in env High Clean environment or container Future: Sandbox Level Control A --sandbox-level flag for granular control is planned but not yet implemented. For now, exploration mode is all-or-nothing with respect to permissions. Best Practices 1. Use Specific Questions # Good: Specific exploration task rounds = [&quot;Search for 'deployment' topics, then report what you found&quot;] # Less effective: Vague request rounds = [&quot;Explore this tool&quot;] 2. Combine with Discussion Mode [session] mode = &quot;discussion&quot; exploration = true [questions] rounds = [ &quot;Each of you explore different aspects of this tool.&quot;, &quot;Share what you found. What patterns do you see?&quot;, &quot;Based on everyone's exploration, what should be prioritized?&quot;, ] 3. Use Moderator for Synthesis [session] exploration = true moderator = true # Synthesize exploration findings 4. Mix Exploration Modes # One agent explores, another analyzes [[agents]] provider = &quot;claude&quot; name = &quot;Explorer&quot; exploration = true [[agents]] provider = &quot;codex&quot; name = &quot;Analyst&quot; exploration = false system_prompt = &quot;Analyze the explorer's findings and suggest improvements.&quot; Troubleshooting Agent Can't Run Commands Verify the CLI tool is installed ( claude --version , codex --version ) Check that the tool being evaluated is in PATH Exploration Too Slow Reduce number of agents Use parallel_agents = false to run sequentially Limit the scope of exploration in your question Agent Runs Wrong Commands Provide more specific instructions in your question Use working_dir to set the right directory Ensure tool name is clear and unambiguous"
    },
    {
      "id": "structured-feedback",
      "title": "Structured Feedback Schemas",
      "tags": "focusgroup feedback json schema",
      "content": "Structured Feedback Schemas Request agents respond with structured JSON matching a predefined schema, enabling automated analysis and aggregation of feedback. Quick Start # Use a built-in schema preset focusgroup ask &quot;Rate this tool&quot; -x &quot;mytool --help&quot; --schema rating # Output includes structured_data in JSON focusgroup ask &quot;Pros and cons?&quot; -x &quot;mytool --help&quot; --schema pros-cons -o json Built-in Schema Presets rating Simple numeric rating with explanation. { &quot;rating&quot;: 4, &quot;reasoning&quot;: &quot;Clear help output, intuitive flags&quot; } pros-cons List positive and negative aspects. { &quot;pros&quot;: [&quot;Fast execution&quot;, &quot;Good error messages&quot;], &quot;cons&quot;: [&quot;Missing documentation&quot;], &quot;summary&quot;: &quot;Solid tool with room for improvement&quot; } review Comprehensive review with rating, pros, cons, and suggestions. { &quot;rating&quot;: 4, &quot;pros&quot;: [&quot;Intuitive CLI&quot;, &quot;Good defaults&quot;], &quot;cons&quot;: [&quot;Verbose output&quot;], &quot;suggestions&quot;: [&quot;Add --quiet flag&quot;, &quot;Support JSON output&quot;] } How It Works Prompt Injection : Schema instructions are appended to your question Agent Response : Agents are instructed to respond with valid JSON Response Parsing : Focusgroup extracts JSON from the response Output : Both raw response and structured data are available JSON Output Format When using --schema with -o json , responses include structured_data : { &quot;rounds&quot;: [{ &quot;question&quot;: &quot;Rate this tool&quot;, &quot;responses&quot;: [{ &quot;agent_name&quot;: &quot;Agent-1&quot;, &quot;provider&quot;: &quot;claude-cli&quot;, &quot;response&quot;: &quot;{\\&quot;rating\\&quot;: 4, \\&quot;reasoning\\&quot;: \\&quot;Good tool\\&quot;}&quot;, &quot;structured_data&quot;: { &quot;rating&quot;: 4, &quot;reasoning&quot;: &quot;Good tool&quot; } }] }] } Custom Schemas (TOML Config) Define custom schemas in your config file: [session] name = &quot;Custom Review&quot; [session.feedback_schema] include_raw_response = true [[session.feedback_schema.fields]] name = &quot;usability&quot; type = &quot;integer&quot; min_value = 1 max_value = 10 description = &quot;How easy is the tool to use?&quot; [[session.feedback_schema.fields]] name = &quot;would_recommend&quot; type = &quot;boolean&quot; description = &quot;Would you recommend this tool to other agents?&quot; [[session.feedback_schema.fields]] name = &quot;improvements&quot; type = &quot;list&quot; description = &quot;Suggested improvements&quot; required = false Field Types Type JSON Type Description integer number Numeric values (supports min_value , max_value ) string string Free text list array Array of strings boolean boolean True/false Response Parsing Focusgroup handles various response formats: Pure JSON : Response is just the JSON object Code blocks : JSON in markdown ```json ... ``` blocks Embedded : JSON object anywhere in the response text If JSON parsing fails, structured_data will be null and the raw response is preserved. Use Cases Automated Analysis # Collect structured reviews from multiple agents focusgroup ask &quot;Review this CLI&quot; -x &quot;mytool --help&quot; --schema review -o json -n 5 | \\ jq '[.rounds[0].responses[].structured_data.rating] | add / length' # Average rating across 5 agents Trend Tracking Compare structured feedback across sessions: focusgroup logs export --json | jq '.rounds[].responses[].structured_data' CI/CD Integration # Fail if average rating is below threshold rating=$(focusgroup ask &quot;Rate this tool&quot; -x &quot;mytool --help&quot; --schema rating -o json -q | \\ jq '[.rounds[0].responses[].structured_data.rating // 0] | add / length') if (( $(echo &quot;$rating &lt; 3&quot; | bc -l) )); then echo &quot;Tool rating too low: $rating&quot; exit 1 fi Best Practices Keep schemas simple : Fewer fields = more reliable parsing Use descriptive field descriptions : Helps agents understand expectations Make non-essential fields optional : Set required = false Use --quiet with scripts : Prevents status messages in output Combine with -o json : Structured data is most useful in JSON output See Also JSON Output Formats - Export format details Configuration Reference - Full config options"
    }
  ],
  "metadata": {
    "modes": {
      "title": "Session Modes",
      "tags": [
        "focusgroup",
        "modes",
        "sessions"
      ],
      "path": "modes.html"
    },
    "cli-path-lookup": {
      "title": "CLI Tool PATH Lookup Behavior",
      "tags": [
        "focusgroup",
        "configuration",
        "troubleshooting"
      ],
      "path": "cli-path-lookup.html"
    },
    "configuration": {
      "title": "Configuration Reference",
      "tags": [
        "focusgroup",
        "configuration",
        "reference"
      ],
      "path": "configuration.html"
    },
    "providers": {
      "title": "Agent Providers Guide",
      "tags": [
        "focusgroup",
        "providers",
        "agents"
      ],
      "path": "providers.html"
    },
    "quick-start": {
      "title": "Quick Start",
      "tags": [
        "focusgroup",
        "getting-started"
      ],
      "path": "quick-start.html"
    },
    "json-formats": {
      "title": "JSON Output Formats",
      "tags": [
        "focusgroup",
        "json",
        "api",
        "output"
      ],
      "path": "json-formats.html"
    },
    "exploration": {
      "title": "Exploration Mode",
      "tags": [
        "focusgroup",
        "exploration",
        "security"
      ],
      "path": "exploration.html"
    },
    "structured-feedback": {
      "title": "Structured Feedback Schemas",
      "tags": [
        "focusgroup",
        "feedback",
        "json",
        "schema"
      ],
      "path": "structured-feedback.html"
    }
  }
}